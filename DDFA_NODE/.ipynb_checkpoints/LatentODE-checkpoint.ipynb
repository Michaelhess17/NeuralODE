{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242691cb-4cc9-464b-813e-fa5893aded2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mDrWatson could not find find a project file by recursively checking given `dir` and its parents. Returning `nothing` instead.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m(given dir: /home/michael)\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ DrWatson ~/.julia/packages/DrWatson/rXaRB/src/project_setup.jl:84\u001b[39m\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Package GokuNets not found in current path.\n- Run `import Pkg; Pkg.add(\"GokuNets\")` to install the GokuNets package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package GokuNets not found in current path.\n- Run `import Pkg; Pkg.add(\"GokuNets\")` to install the GokuNets package.",
      "",
      "Stacktrace:",
      " [1] macro expansion",
      "   @ ./loading.jl:1163 [inlined]",
      " [2] macro expansion",
      "   @ ./lock.jl:223 [inlined]",
      " [3] require(into::Module, mod::Symbol)",
      "   @ Base ./loading.jl:1144"
     ]
    }
   ],
   "source": [
    "ENV[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "using DrWatson\n",
    "@quickactivate \"TMLR_experiments\"\n",
    "using GokuNets\n",
    "using TMLR_experiments\n",
    "using Flux\n",
    "\n",
    "GokuNets.linear_scale_C(x) = x*0.2f0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581f674d-b12b-4f94-87f8-aca05efda60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "## Arguments for the train function\n",
    "\n",
    "general_args = Dict(\n",
    "    ## Global model\n",
    "    :model_type => GOKU_attention(),    # base model  \n",
    "\n",
    "    ## Latent Differential Equations\n",
    "    :diffeq => Stoch_Hopf,\n",
    "    :diffeq_args => Dict(:N => 3),                      # optional system and solver arguments\n",
    "    :dt => 0.05f0,                                      # timestep for saving numerical solution\n",
    "\n",
    "    ## Multiple shooting settings  \n",
    "    :multiple_shooting => [true]                # multiple or single shooting training   \n",
    "    :win_len => 100,                                     # window length for the multiple shooting\n",
    "    :continuity_term => 2f0,                            # continuity regularization weight in the muliple shooting loss function        \n",
    "\n",
    "    ## Data\n",
    "    :training_samples => 1000,         # number of samples used for training\n",
    "    :val_samples => 150,                                # number of samples used for validation\n",
    "    :data_path => datadir(\"/home/michael/Code/TMLR_GOKU-UI/Experiments/scripts/Training/Baselines/LatentODE/sims\", \"compass_data.h5\"),\n",
    "                                                        # path to the data file\n",
    "    ## Training params\n",
    "    :epochs => 500,                                   # maximum number of epochs for training\n",
    "    :batch_size => 64,                                  # minibatch size\n",
    "    :seq_len => 500,                                     # approximate sequence length for training samples\n",
    "                                                        # (it may be adjusted to make it compatible with multiple shooting)\n",
    "    :optimizer => AdamW,                                # optimizer\n",
    "    :lr => 0.005251,                                    # base learning rate for the schedule\n",
    "    :decay_of_momentums => (0.9, 0.999),                # decay of momentums\n",
    "    :ϵ => 1.0e-8,                                       # ϵ for ADAM\n",
    "    :weight_decay => 1e-10,                             # weight decay for ADAMW\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    :logging_and_scheduling_period => 9,                # interval for logging and lr schedule, measured in batch count\n",
    "    :lr_scheduler => Cos4Exp,                           # main learning rate scheduler: Cos4Exp or Exp\n",
    "    :min_lr => 0.00001,                                 # lower bound for the learning rate\n",
    "    :warmup => 20*9,                                    # batches of initial linear growth of the lr until reaching the base lr\n",
    "    :patience_lr => 50,                                 # patience (measured in logging_and_scheduling_periods) until reducing the lr for the first time\n",
    "    :patience_lr2 => 2,                                 # patience (measured in logging_and_scheduling_periods) until reducing the lr for the subsequent times\n",
    "    :threshold => 0.01,                                 # dynamic threshold for measuring the new optimum. See https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n",
    "    :period => 50,                                      # period for the cosine/triangle lr schedules\n",
    "    :γ => 0.995,                                        # exponential decay for the CosExp or Exp schedules\n",
    "    :patience => 35,                                   # epochs (measured in logging_and_scheduling_periods) until early stopping of the training\n",
    "\n",
    "    ## Variational settings\n",
    "    :variational => false,                              # deterministic or variational training\n",
    "    :initial_wait_β => 0,\n",
    "    :annealing_γ => 0.999999,\n",
    "    :annealing_length => 0,\n",
    "\n",
    "    ## Networks sizes and activation functions\n",
    "    :hidden_dim_resnet => 128,\n",
    "    :rnn_input_dim => 128,\n",
    "    :rnn_output_dim => 32,\n",
    "    :latent_dim_z₀ => 32,\n",
    "    :latent_dim_θ => 64,\n",
    "    :latent_to_diffeq_dim => 200,\n",
    "    :general_activation => mish,\n",
    "    :z₀_activation => identity,\n",
    "    :θ_activation => σ,\n",
    "    :output_activation => identity,\n",
    "    :init => Flux.kaiming_uniform(gain = 1/sqrt(3)),\n",
    "\n",
    "    ## Other\n",
    "    :seed => 42,                             # random seed\n",
    "    :cuda => false,                                     # GPU usage if available\n",
    "    :verbose => true,\n",
    "    :experiments_name => \"GOKU_data_scaling\",\n",
    "    :name => \"run1\",\n",
    "    :comments => \"large param ranges in the model diffeq but small ranges in the data\",\n",
    "    :resume => false,\n",
    "    :save_checkpoints => false,\n",
    "    :save_output => true,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d8ba89-ef69-4203-95c9-d1726c860dba",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: GOKU_attention not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: GOKU_attention not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[3]:4"
     ]
    }
   ],
   "source": [
    "dicts = dict_list(general_args);\n",
    "\n",
    "map(training_pipeline, dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e226b2-2dcd-4cd2-af5a-b9311bf100fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
