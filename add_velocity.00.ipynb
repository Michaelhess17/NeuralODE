{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, jit\n",
    "# set 64-bit mode\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "from scipy import signal, interpolate, stats\n",
    "# sys.path.append(\"/home/michael/Synology/Desktop/Data/Python/NeuralODE/DDFA_NODE/\")\n",
    "from ddfa_node import embed_data, takens_embedding, change_trial_length, split_data, get_aics, get_λs, phaser, stats as statistics, jax_utils\n",
    "import ddfa_node\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = jnp.load(\"outputs/VDP_oscillators.npy\")\n",
    "data = data.reshape(data.shape[0]*data.shape[1], data.shape[2], data.shape[3])\n",
    "for idx in range(data.shape[0]):\n",
    "    plt.plot(data[idx, :, 0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_loc = \"data/Modelingandanalysis-latest/\"\n",
    "files = glob.glob(data_loc + \"/**/*K0.csv\", recursive=True)\n",
    "df = pd.read_csv(files[0])\n",
    "df[\"Subject\"] = 0\n",
    "for idx, file in enumerate(files[1:]):\n",
    "    df1 = pd.read_csv(file)\n",
    "    df1[\"Subject\"] = idx + 1\n",
    "    df = pd.concat((df, df1))\n",
    "    \n",
    "df = df[[\"hip_flexion_r\", \"hip_flexion_l\", \"knee_angle_r\", \"knee_angle_l\", \"ankle_angle_r\", \"ankle_angle_l\", \"Subject\"]]\n",
    "window_length = 30\n",
    "polyorder = 5\n",
    "feats = [\"hip_flexion_l\", \"knee_angle_l\", \"ankle_angle_l\", \"hip_flexion_r\",  \"knee_angle_r\", \"ankle_angle_r\"]\n",
    "n_subjects = len(df[\"Subject\"].unique())\n",
    "total_steps = len(df)\n",
    "data = np.zeros((n_subjects, total_steps // n_subjects, len(feats) * 2)) # 2x the features for velocity\n",
    "for subject in range(n_subjects):\n",
    "    data[subject, :, :len(feats)] = savgol_filter(df[df[\"Subject\"] == subject][feats].to_numpy(), window_length=window_length, polyorder=polyorder, axis=0)\n",
    "    data[subject, :, len(feats):] = savgol_filter(df[df[\"Subject\"] == subject][feats].to_numpy(), window_length=window_length, polyorder=polyorder, axis=0, deriv=1)\n",
    "\n",
    "# Standardize the data\n",
    "data = (data - np.mean(data, axis=1)[:, None, :]) / np.std(data, axis=1)[:, None, :]\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 50\n",
    "polyorder = 5\n",
    "data = jnp.load(\"/home/michael/Synology/Julia/data/VDP_SDEs.npy\")[25:-20, ::2, :1]\n",
    "feats = data.shape[-1]\n",
    "n_trials = data.shape[0]\n",
    "new_data = np.zeros((n_trials, data.shape[1], data.shape[2]))\n",
    "for trial in range(n_trials):\n",
    "    new_data[trial, :, :feats] = savgol_filter(data[trial, :, :], window_length=window_length, polyorder=polyorder, axis=0)\n",
    "    # new_data[trial, :, feats:] = savgol_filter(data[trial, :, :], window_length=window_length, polyorder=polyorder, axis=0, deriv=1)\n",
    "\n",
    "# Standardize the data\n",
    "new_data = (new_data - jnp.mean(new_data, axis=1)[:, None, :]) / jnp.std(new_data, axis=1)[:, None, :]\n",
    "data = new_data\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[11, 0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_tde, τ, k = embed_data(data[:, :, :1])\n",
    "τ = 32\n",
    "k = 4\n",
    "data_tde = takens_embedding(data[:, :, :1], τ, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_tde[0, 0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The data have shape {data_tde.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , static_argnames=[\"timesteps_per_trial\", \"skip\", \"t1\", \"width_size\", \"hidden_size\", \"ode_size\", \"depth\", \"batch_size\", \"seed\", \"print_every\", \"length_strategy\", \"lr_strategy\", \"seeding_strategy\", \"steps_strategy\", \"plot\",\"k\"\n",
    "ts, ys, model = jax_utils.train_NODE(\n",
    "    # model=model,\n",
    "    data_tde,\n",
    "    timesteps_per_trial=300,\n",
    "    t1=3.0,\n",
    "    width_size=128,\n",
    "    hidden_size=256,\n",
    "    ode_size=8,\n",
    "    depth=3,\n",
    "    batch_size=128,\n",
    "    seed=6969,\n",
    "    lr_strategy=(1e-3,),\n",
    "    steps_strategy=(70000, 30000, 25000),\n",
    "    length_strategy=(1,),\n",
    "    skip_strategy=(10,),\n",
    "    seeding_strategy=(1/3,),\n",
    "    plot=True,\n",
    "    print_every=1000,\n",
    "    k=1,\n",
    "    linear=False,\n",
    "    plot_fn=None,\n",
    "    # k=max_power+2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model\n",
    "import equinox as eqx\n",
    "eqx.tree_serialise_leaves(\"outputs/vdp_model.eqx\", model)\n",
    "\n",
    "# deserialize the model\n",
    "model2 = eqx.tree_deserialise_leaves(\"outputs/vdp_model.eqx\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ts = jnp.linspace(0, 30, 1500)\n",
    "seeding_steps = 100\n",
    "out = jax.vmap(model, in_axes=(None, 0))(new_ts, data_tde[:, 0:seeding_steps, :])\n",
    "jnp.save(\"outputs/gen_vdp_data.npy\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(data.shape[0]):\n",
    "    plt.scatter(new_ts, data[idx, 0:new_ts.shape[0]*2:2, 0], label=\"True\",s=5, c=\"black\")\n",
    "    plt.scatter(new_ts, out[idx, :, 0], label=\"Generated\",s=5, c=\"red\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "@jit\n",
    "def normalize_signal(signal):\n",
    "    \"\"\"Normalize a single signal\"\"\"\n",
    "    mean = jnp.mean(signal)\n",
    "    std = jnp.std(signal)\n",
    "    return (signal - mean) / std\n",
    "\n",
    "# Vectorized version for multiple signals\n",
    "vnormalize = vmap(normalize_signal, in_axes=0, out_axes=0)\n",
    "\n",
    "@jit\n",
    "def euclidean_distance(x, y):\n",
    "    \"\"\"Compute euclidean distance between two points\"\"\"\n",
    "    return jnp.sqrt(jnp.sum((x - y) ** 2))\n",
    "\n",
    "@jit\n",
    "def cross_correlation(signal1, signal2):\n",
    "    \"\"\"Compute normalized cross-correlation for single pair of signals\"\"\"\n",
    "    # Normalize signals\n",
    "    s1_norm = normalize_signal(signal1)\n",
    "    s2_norm = normalize_signal(signal2)\n",
    "    \n",
    "    # Calculate cross-correlation\n",
    "    xcorr = jnp.correlate(s1_norm, s2_norm, mode='full')\n",
    "    \n",
    "    # Return maximum correlation coefficient\n",
    "    return jnp.max(jnp.abs(xcorr)) / signal1.shape[0]\n",
    "\n",
    "# Vectorized version for multiple signal pairs\n",
    "vcross_correlation = vmap(cross_correlation, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "@jit\n",
    "def fft_comparison(signal1, signal2):\n",
    "    \"\"\"Compare frequency components of two signals\"\"\"\n",
    "    # Compute FFTs\n",
    "    fft1 = jnp.fft.fft(signal1)\n",
    "    fft2 = jnp.fft.fft(signal2)\n",
    "    \n",
    "    # Compare magnitude spectra\n",
    "    mag1 = jnp.abs(fft1)\n",
    "    mag2 = jnp.abs(fft2)\n",
    "    \n",
    "    # Normalize\n",
    "    mag1_norm = mag1 / jnp.max(mag1)\n",
    "    mag2_norm = mag2 / jnp.max(mag2)\n",
    "    \n",
    "    return jnp.mean(jnp.abs(mag1_norm - mag2_norm))\n",
    "\n",
    "# Vectorized version for multiple signal pairs\n",
    "vfft_comparison = vmap(fft_comparison, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "@jit\n",
    "def simplified_dtw(signal1, signal2, window_size=10):\n",
    "    \"\"\"\n",
    "    Simplified DTW implementation that's JAX-friendly\n",
    "    Uses a constrained window for better performance\n",
    "    \"\"\"\n",
    "    n, m = signal1.shape[0], signal2.shape[0]\n",
    "    \n",
    "    # Initialize cost matrix\n",
    "    cost_matrix = jnp.full((n + 1, m + 1), jnp.inf)\n",
    "    cost_matrix = cost_matrix.at[0, 0].set(0)\n",
    "    \n",
    "    def scan_fn(prev_row, curr_idx):\n",
    "        curr_row = jnp.full(m + 1, jnp.inf)\n",
    "        curr_row = curr_row.at[0].set(jnp.inf)\n",
    "        \n",
    "        i = curr_idx + 1\n",
    "        \n",
    "        def update_cell(j):\n",
    "            cost = euclidean_distance(signal1[i-1], signal2[j-1])\n",
    "            new_cost = cost + jnp.min(jnp.array([\n",
    "                prev_row[j],\n",
    "                prev_row[j-1],\n",
    "                curr_row[j-1]\n",
    "            ]))\n",
    "            \n",
    "            return jax.lax.cond(\n",
    "                jnp.abs(i - j) <= window_size,\n",
    "                lambda _: new_cost,\n",
    "                lambda _: jnp.inf,\n",
    "                operand=None\n",
    "            )\n",
    "        \n",
    "        j_indices = jnp.arange(1, m + 1)\n",
    "        costs = vmap(update_cell)(j_indices)\n",
    "        curr_row = curr_row.at[1:].set(costs)\n",
    "        \n",
    "        return curr_row, curr_row\n",
    "    \n",
    "    # Scan over all rows\n",
    "    final_row, _ = jax.lax.scan(\n",
    "        scan_fn,\n",
    "        cost_matrix[0],\n",
    "        jnp.arange(n)\n",
    "    )\n",
    "    \n",
    "    # Return just the final DTW distance\n",
    "    return final_row[-1]\n",
    "\n",
    "# Vectorized version for multiple signal pairs\n",
    "vsimplified_dtw = vmap(simplified_dtw, in_axes=(0, 0), out_axes=0)\n",
    "\n",
    "@jit\n",
    "def comprehensive_comparison(signal1, signal2):\n",
    "    \"\"\"Compute all metrics for a single pair of signals\"\"\"\n",
    "    return {\n",
    "        'dtw': simplified_dtw(signal1, signal2),\n",
    "        'xcorr': cross_correlation(signal1, signal2),\n",
    "        'fft': fft_comparison(signal1, signal2)\n",
    "    }\n",
    "\n",
    "# Example usage with batched data\n",
    "def batch_compare_signals(signals1, signals2):\n",
    "    \"\"\"\n",
    "    Compare multiple pairs of signals at once\n",
    "    \n",
    "    Args:\n",
    "        signals1: Array of shape (batch_size, signal_length)\n",
    "        signals2: Array of shape (batch_size, signal_length)\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'dtw': vsimplified_dtw(signals1, signals2),\n",
    "        'xcorr': vcross_correlation(signals1, signals2),\n",
    "        'fft': vfft_comparison(signals1, signals2)\n",
    "    }\n",
    "    \n",
    "\n",
    "batch_signals1 = jnp.copy(data[:, 0:new_ts.shape[0]*2:2, 0])  # shape: (batch_size, signal_length)\n",
    "batch_signals2 = jnp.copy(out[:, :, 1])  # shape: (batch_size, signal_length)\n",
    "\n",
    "batch_results = batch_compare_signals(batch_signals1, batch_signals2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert batch results to a DataFrame for easier plotting\n",
    "def plot_comparison_results(batch_results, figsize=(15, 5)):\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'DTW': batch_results['dtw'],\n",
    "        'Cross-correlation': batch_results['xcorr'],\n",
    "        'FFT Similarity': batch_results['fft'],\n",
    "        'Trial': range(len(batch_results['dtw']))\n",
    "    })\n",
    "    \n",
    "    # Create subplot for each metric\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # DTW plot (lower is better)\n",
    "    sns.boxplot(y='DTW', data=results_df, ax=axes[0])\n",
    "    sns.stripplot(y='DTW', data=results_df, color='red', alpha=0.3, ax=axes[0])\n",
    "    axes[0].set_title('DTW Distance\\n(lower is better)')\n",
    "    \n",
    "    # Cross-correlation plot (higher is better)\n",
    "    sns.boxplot(y='Cross-correlation', data=results_df, ax=axes[1])\n",
    "    sns.stripplot(y='Cross-correlation', data=results_df, color='red', alpha=0.3, ax=axes[1])\n",
    "    axes[1].set_title('Cross-correlation\\n(higher is better)')\n",
    "    \n",
    "    # FFT Similarity plot (lower is better)\n",
    "    sns.boxplot(y='FFT Similarity', data=results_df, ax=axes[2])\n",
    "    sns.stripplot(y='FFT Similarity', data=results_df, color='red', alpha=0.3, ax=axes[2])\n",
    "    axes[2].set_title('FFT Difference\\n(lower is better)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(results_df.describe())\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Plot the results\n",
    "results_df = plot_comparison_results(batch_results)\n",
    "\n",
    "# Optional: Plot correlation between metrics\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(results_df[['DTW', 'Cross-correlation', 'FFT Similarity']].corr(), \n",
    "            annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation between metrics')\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plot worst and best matches\n",
    "def plot_extreme_cases(batch_signals1, batch_signals2, results_df, metric='DTW'):\n",
    "    best_idx = results_df[metric].argmin() if metric in ['DTW', 'FFT Similarity'] else results_df[metric].argmax()\n",
    "    worst_idx = results_df[metric].argmax() if metric in ['DTW', 'FFT Similarity'] else results_df[metric].argmin()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Best match\n",
    "    ax1.plot(batch_signals1[best_idx], label='Original', alpha=0.7, c=\"black\")\n",
    "    ax1.plot(batch_signals2[best_idx], label='Generated', alpha=0.7, c=\"red\")\n",
    "    ax1.set_title(f'Best Match (by {metric})')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Worst match\n",
    "    ax2.plot(batch_signals1[worst_idx], label='Original', alpha=0.7, c=\"black\")\n",
    "    ax2.plot(batch_signals2[worst_idx], label='Generated', alpha=0.7, c=\"red\")\n",
    "    ax2.set_title(f'Worst Match (by {metric})')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot extreme cases for each metric\n",
    "for metric in ['DTW', 'Cross-correlation', 'FFT Similarity']:\n",
    "    plot_extreme_cases(batch_signals1, batch_signals2, results_df, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
